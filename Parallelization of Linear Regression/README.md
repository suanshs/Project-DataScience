## Parallelization of Linear Regression 


Improvements in the computational power and in-memory storage of personal computers along with the ease of scaling up thanks to cloud computing services have opened data analytics to a broader audience. This in turn played an important role in building the excitement around Big Data and machine learning. Today, anyone can rapidly prototype any model they choose from a popular machine learning library without needing to worrying about the efficiency of their implementation. However, to generate production quality code inefficiencies cannot be easily overlooked. One easy way to improve the efficiency is using parallelization.
Linear regression is one of the most commonly used models in data analytics because it is easy to implement and interpret. Additionally, it has been studied extensively, hence its strengths and weaknesses are well-known. To build a linear regression, the user needs to determine which variable they want to model, the target variable, and the variables they want to use, independent variables, to explain this chosen variable. Once this selection is done, they can reach conclusions about the target variable assuming they keep linear regression’s limitations in mind.
Due to its wide use, implementing a scalable linear regression is an important addition to a Data Scientist’s arsenal. The aim of this project is to show the benefits of parallelizing linear regression and showcase different approaches to achieving this task.

The detailed information about the literature survey, our methodology, implementation, results and conclusion can be found in the documentation - *ParallelizationOfLinearRegression.docx* https://github.com/suanshs/Project-DataScience/blob/master/Parallelization%20of%20Linear%20Regression/ParallelizationOfLinearRegression.docx .

